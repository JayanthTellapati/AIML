Model Evaluation & Validation




1.k-fold_cross_validation.py






[10:47 AM] Poornima (Guest)
# Implementing k-Fold Cross-Validation
import torch
from sklearn.datasets import load_iris
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader, TensorDataset
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()
# Define a simple neural network model
class SimpleNN(torch.nn.Module):
    def __init__(self, input_size, num_classes):
        super(SimpleNN, self).__init__()
        self.fc = torch.nn.Linear(input_size, num_classes)
    def forward(self, x):
        return self.fc(x)
# Function to train the model
def train_model(model, criterion, optimizer, dataloader):
    model.train()
    for inputs, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
# Function to evaluate the model
def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.numpy())
            all_labels.extend(labels.numpy())
    return accuracy_score(all_labels, all_preds)
# k-fold cross-validation
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)
fold_accuracies = []
for train_index, val_index in kf.split(X_tensor):
    X_train, X_val = X_tensor[train_index], X_tensor[val_index]
    y_train, y_val = y_tensor[train_index], y_tensor[val_index]
    # Create dataloaders
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
    # Initialize model, loss function, and optimizer
    model = SimpleNN(input_size=X_tensor.shape[1], num_classes=len(torch.unique(y_tensor)))
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    # Train the model
    train_model(model, criterion, optimizer, train_loader)
    # Evaluate the model
    accuracy = evaluate_model(model, val_loader)
    fold_accuracies.append(accuracy)
    print(f'Fold accuracy: {accuracy:.4f}')
# Average accuracy across all folds
average_accuracy = sum(fold_accuracies) / k
print(f'Average accuracy: {average_accuracy:.4f}')


 





2.stratified_cross_validation.py




import torch
from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader, TensorDataset

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()


# Define a simple neural network model
class SimpleNN(torch.nn.Module):
    def _init_(self, input_size, num_classes):
        super(SimpleNN, self)._init_()
        self.fc = torch.nn.Linear(input_size, num_classes)

    def forward(self, x):
        return self.fc(x)

# Function to train the model
def train_model(model, criterion, optimizer, dataloader):
    model.train()
    for inputs, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Function to evaluate the model
def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.numpy())
            all_labels.extend(labels.numpy())
    return accuracy_score(all_labels, all_preds)

# k-fold cross-validation
k = 5
skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
fold_accuracies = []

for train_index, val_index in skf.split(X_tensor, y_tensor):
    X_train, X_val = X_tensor[train_index], X_tensor[val_index]
    y_train, y_val = y_tensor[train_index], y_tensor[val_index]

    # Create dataloaders
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

    # Initialize model, loss function, and optimizer
    model = SimpleNN(input_size=X_tensor.shape[1], num_classes=len(torch.unique(y_tensor)))
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Train the model
    train_model(model, criterion, optimizer, train_loader)

    # Evaluate the model
    accuracy = evaluate_model(model, val_loader)
    fold_accuracies.append(accuracy)
    print(f'Fold accuracy: {accuracy:.4f}')

# Average accuracy across all folds
average_accuracy = sum(fold_accuracies) / k
print(f'Average accuracy: {average_accuracy:.4f}')














3.MAE_MSE_RMSE_RSQRD.py






[12:09 PM] Poornima (Guest)

import torch
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
# Calculating MAE
# Load the California housing dataset
data = fetch_california_housing()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).float()
# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor,
                                                    test_size=0.2, random_state=42)
# Define a simple neural network model for regression
class SimpleNN(torch.nn.Module):
    def __init__(self, input_size):
        super(SimpleNN, self).__init__()
        self.fc = torch.nn.Linear(input_size, 1)
    def forward(self, x):
        return self.fc(x)
# Function to calculate Mean Absolute Error (MAE)
def calculate_mae(y_true, y_pred):
    return torch.mean(torch.abs(y_true - y_pred)).item()
# Initialize model, loss function, and optimizer
model = SimpleNN(input_size=X_tensor.shape[1])
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
# Create dataloaders
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
# Train the model
model.train()
for epoch in range(100):  # Train for 100 epochs
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
# Evaluate the model
model.eval()
y_pred = []
with torch.no_grad():
    for inputs, _ in test_loader:
        outputs = model(inputs)
        y_pred.append(outputs)
y_pred = torch.cat(y_pred, dim=0)
# Calculate MAE
mae = calculate_mae(y_test, y_pred)
print(f'Mean Absolute Error (MAE): {mae:.4f}')

# Calculating MSE and RMSE
# Function to calculate Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
def calculate_mse_rmse(y_true, y_pred):
    mse = torch.mean((y_true - y_pred) ** 2).item()
    rmse = torch.sqrt(torch.mean((y_true - y_pred) ** 2)).item()
    return mse, rmse
# Calculate MSE and RMSE
mse, rmse = calculate_mse_rmse(y_test, y_pred)
print(f'Mean Squared Error (MSE): {mse:.4f}')
print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')
 
 


[12:45 PM] Poornima (Guest)
# Calculating R-squared
# Function to calculate R-squared (R²)
def calculate_r2(y_true, y_pred):
    ss_total = torch.sum((y_true - torch.mean(y_true)) ** 2).item()
    ss_residual = torch.sum((y_true - y_pred) ** 2).item()
    r2 = 1 - (ss_residual / ss_total)
    return r2
# Calculate R-squared
r2 = calculate_r2(y_test, y_pred)
print(f'R-squared (R²): {r2:.4f}')
 
 











4.accuracy.py







[3:17 PM] Poornima (Guest)
# Calculating Accuracy
import numpy as np
import torch
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()
# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)
# Define a simple neural network model for classification
class SimpleNN(torch.nn.Module):
    def __init__(self, input_size, num_classes):
        super(SimpleNN, self).__init__()
        self.fc = torch.nn.Linear(input_size, num_classes)
    def forward(self, x):
        return self.fc(x)

# Initialize model, loss function, and optimizer
model = SimpleNN(input_size=X_tensor.shape[1], num_classes=len(torch.unique(y_tensor)))
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
# Create dataloaders
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
# Train the model
model.train()
for epoch in range(100):  # Train for 100 epochs
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
# Evaluate the model
model.eval()
y_pred = []
with torch.no_grad():
    for inputs, _ in test_loader:
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        y_pred.extend(predicted.numpy())
y_pred = torch.tensor(y_pred)

# Function to calculate accuracy
def calculate_accuracy(y_true, y_pred):
    correct = (y_true == y_pred).float()
    accuracy = correct.sum() / len(correct)
    return accuracy.item()
# Calculate accuracy
accuracy = calculate_accuracy(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}')

# Precision, Recall, and F1-Score
from sklearn.metrics import precision_score, recall_score, f1_score
# Calculate precision, recall, and F1-score with zero_division parameter
precision = precision_score(y_test.numpy(), y_pred.numpy(), average='macro', zero_division=0)
recall = recall_score(y_test.numpy(), y_pred.numpy(), average='macro', zero_division=0)
f1 = f1_score(y_test.numpy(), y_pred.numpy(), average='macro', zero_division=0)
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-Score: {f1:.4f}')
 









[3:48 PM] Poornima (Guest)
# ROC Curve and AUC
from sklearn.metrics import roc_curve, auc
# Calculate probabilities
model.eval()
y_prob = []
with torch.no_grad():
    for inputs, _ in test_loader:
        outputs = model(inputs)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        y_prob.extend(probabilities.numpy())
y_prob = np.array(y_prob)  # Convert list of NumPy arrays to a single NumPy array
y_prob = torch.from_numpy(y_prob)  # Convert the NumPy array to a PyTorch tensor
# Binarize the output for ROC
y_test_binarized = torch.nn.functional.one_hot(y_test).numpy()
y_prob = y_prob.numpy()
# Plot ROC curve and calculate AUC for each class
plt.figure(figsize=(10, 8))
for i in range(len(data.target_names)):
    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Class {data.target_names[i]} (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()
 














5.nested_cross_validation.py








[5:03 PM] Poornima (Guest)
# Nested Cross - Validation
import torch
from sklearn.datasets import load_iris
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import ParameterGrid
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()

# Define a simple neural network model for classification
class SimpleNN(torch.nn.Module):
    def __init__(self, input_size, num_classes, hidden_size=16):
        super(SimpleNN, self).__init__()
        self.fc1 = torch.nn.Linear(input_size, hidden_size)
        self.fc2 = torch.nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Function to train the model
def train_model(model, criterion, optimizer, dataloader):
    model.train()
    for inputs, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Function to evaluate the model
def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.numpy())
            all_labels.extend(labels.numpy())
    return accuracy_score(all_labels, all_preds)

# Define parameter grid for hyperparameter tuning
param_grid = {
    'hidden_size': [8, 16, 32],
    'lr': [0.01, 0.001]
}
# Nested cross-validation
outer_k = 5
inner_k = 3
outer_cv = KFold(n_splits=outer_k, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=inner_k, shuffle=True, random_state=42)
outer_accuracies = []
for train_idx, test_idx in outer_cv.split(X_tensor):
    X_train, X_test = X_tensor[train_idx], X_tensor[test_idx]
    y_train, y_test = y_tensor[train_idx], y_tensor[test_idx]
    best_params = None
    best_score = 0
    for params in ParameterGrid(param_grid):
        inner_scores = []
        for inner_train_idx, inner_val_idx in inner_cv.split(X_train):
            X_inner_train, X_inner_val = X_train[inner_train_idx], X_train[inner_val_idx]
            y_inner_train, y_inner_val = y_train[inner_train_idx], y_train[inner_val_idx]
            train_dataset = TensorDataset(X_inner_train, y_inner_train)
            val_dataset = TensorDataset(X_inner_val, y_inner_val)
            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
            model = SimpleNN(input_size=X_tensor.shape[1],
                             num_classes=len(torch.unique(y_tensor)),
                             hidden_size=params['hidden_size'])
            criterion = torch.nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])
            train_model(model, criterion, optimizer, train_loader)
            inner_score = evaluate_model(model, val_loader)
            inner_scores.append(inner_score)
        avg_inner_score = sum(inner_scores) / inner_k
        if avg_inner_score > best_score:
            best_score = avg_inner_score
            best_params = params
    # Train on the entire training set with the best hyperparameters
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
    model = SimpleNN(input_size=X_tensor.shape[1], num_classes=len(torch.unique(y_tensor)),
                     hidden_size=best_params['hidden_size'])
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])
    train_model(model, criterion, optimizer, train_loader)
    outer_score = evaluate_model(model, test_loader)
    outer_accuracies.append(outer_score)
    print(f'Outer fold accuracy: {outer_score:.4f}')
# Average accuracy across all outer folds
average_outer_accuracy = sum(outer_accuracies) / outer_k
print(f'Average outer fold accuracy: {average_outer_accuracy:.4f}')



 






6.bootstrapping.py






# Implementing Bootstrapping
import torch
from sklearn.datasets import load_iris
from sklearn.utils import resample
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()

# Define a simple neural network model for classification
class SimpleNN(torch.nn.Module):
    def _init_(self, input_size, num_classes, hidden_size=16):
        super(SimpleNN, self)._init_()
        self.fc1 = torch.nn.Linear(input_size, hidden_size)
        self.fc2 = torch.nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Function to train the model
def train_model(model, criterion, optimizer, dataloader):
    model.train()
    for inputs, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Function to evaluate the model
def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.numpy())
            all_labels.extend(labels.numpy())
    return accuracy_score(all_labels, all_preds)

# Bootstrapping
n_bootstraps = 100
boot_accuracies = []

for _ in range(n_bootstraps):
    # Resample the data
    X_resampled, y_resampled = resample(X_tensor.numpy(), y_tensor.numpy(), replace=True, random_state=None)
    X_resampled = torch.from_numpy(X_resampled)
    y_resampled = torch.from_numpy(y_resampled)

    # Identify out-of-bag samples
    oob_indices = list(set(range(len(y_tensor))) - set(np.unique(resample(range(len(y_tensor)), replace=True))))
    X_oob = X_tensor[oob_indices]
    y_oob = y_tensor[oob_indices]

    # Create dataloaders
    train_dataset = TensorDataset(X_resampled, y_resampled)
    oob_dataset = TensorDataset(X_oob, y_oob)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    oob_loader = DataLoader(oob_dataset, batch_size=16, shuffle=False)

    # Initialize model, loss function, and optimizer
    model = SimpleNN(input_size=X_tensor.shape[1], num_classes=len(torch.unique(y_tensor)), hidden_size=16)
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Train the model
    train_model(model, criterion, optimizer, train_loader)

    # Evaluate the model on OOB samples
    oob_accuracy = evaluate_model(model, oob_loader)
    boot_accuracies.append(oob_accuracy)

# Average accuracy and confidence intervals
average_boot_accuracy = np.mean(boot_accuracies)
confidence_interval = (np.percentile(boot_accuracies, 2.5), np.percentile(boot_accuracies, 97.5))

print(f'Bootstrapped average accuracy: {average_boot_accuracy:.4f}')
print(f'95% confidence interval: {confidence_interval}')
