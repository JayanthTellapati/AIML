Adv Sup Learning Tech




1.bagging.py





[10:15 AM] Poornima (Guest)
# Implementing Bagging
import numpy as np
import torch
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch import nn, optim

# Load the dataset
cali = load_iris()
X, y = cali.data, cali.target
# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

# Define a simple neural network
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(X_train.shape[1], 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Function to train a single model
def train_model(model, X_train, y_train, epochs=100):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs.squeeze(), y_train)
        loss.backward()
        optimizer.step()
    return model

# Train multiple models on bootstrapped samples
def bootstrap_sample(X, y):
    indices = torch.randint(0, len(X), (len(X),))
    return X[indices], y[indices]

num_models = 2
models = []
for _ in range(num_models):
    X_bootstrap, y_bootstrap = bootstrap_sample(X_train, y_train)
    model = Net()
    model = train_model(model, X_bootstrap, y_bootstrap)
    models.append(model)

# Function to make predictions with an ensemble of models
def predict(models, X):
    predictions = [model(X).detach().numpy() for model in models]
    avg_predictions = np.mean(predictions, axis=0)
    return torch.tensor(avg_predictions)

# Make predictions on the test set
y_pred = predict(models, X_test)

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the dataset
iris = load_iris()
X, y = iris.data, iris.target
# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

# Define a simple neural network for classification
class ClassifierNet(nn.Module):
    def __init__(self):
        super(ClassifierNet, self).__init__()
        self.fc1 = nn.Linear(X_train.shape[1], 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 3)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Function to train a single model
def train_model(model, X_train, y_train, epochs=100):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
    return model

# Train multiple models on bootstrapped samples
num_models = 2
models = []
for _ in range(num_models):
    X_bootstrap, y_bootstrap = bootstrap_sample(X_train, y_train)
    model = ClassifierNet()
    model = train_model(model, X_bootstrap, y_bootstrap)
    models.append(model)

# Function to make predictions with an ensemble of models
def predict(models, X):
    predictions = [model(X).detach().numpy() for model in models]
    avg_predictions = np.mean(predictions, axis=0)
    return torch.tensor(np.argmax(avg_predictions, axis=1))

# Make predictions on the test set
y_pred = predict(models, X_test)

from sklearn.metrics import accuracy_score, classification_report
# Evaluate the ensemble model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")



 
[12:21 PM] Poornima (Guest)
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# Load the dataset
data = load_breast_cancer()
X, y = data.data, data.target
# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils import resample
# Function to perform bagging and return trained classifiers
def bagging_classifiers(X, y, n_samples=100):
    classifiers = []
    for _ in range(n_samples):
        X_resampled, y_resampled = resample(X, y, random_state=np.random.randint(10000))
        #print(f'Res count : {len(X_resampled)}')
        clf1 = LogisticRegression(max_iter=10000)
        clf2 = DecisionTreeClassifier()
        clf1.fit(X_resampled, y_resampled)
        clf2.fit(X_resampled, y_resampled)
        classifiers.append((clf1, clf2))
    return classifiers
# Train classifiers
classifiers = bagging_classifiers(X_train, y_train, n_samples=10)
# Function to predict using the trained classifiers
def predict_with_classifiers(classifiers, X):
    predictions1 = np.zeros((len(classifiers), X.shape[0]))
    predictions2 = np.zeros((len(classifiers), X.shape[0]))
    for i, (clf1, clf2) in enumerate(classifiers):
        predictions1[i, :] = clf1.predict(X)
        predictions2[i, :] = clf2.predict(X)
    return predictions1, predictions2
# Make predictions on the test set
predictions1, predictions2 = predict_with_classifiers(classifiers, X_test)
# Function to aggregate predictions by voting
def aggregate_predictions(predictions):
    return np.round(np.mean(predictions, axis=0))
# Aggregate predictions for each classifier
aggregated_predictions1 = aggregate_predictions(predictions1)
aggregated_predictions2 = aggregate_predictions(predictions2)
# Combine predictions from both classifiers
combined_predictions = np.round((aggregated_predictions1 + aggregated_predictions2) / 2)

# Function to evaluate the model
def evaluate_model(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    return accuracy, precision, recall, f1
# Evaluate individual classifiers
accuracy1, precision1, recall1, f11 = evaluate_model(y_test, aggregated_predictions1)
accuracy2, precision2, recall2, f12 = evaluate_model(y_test, aggregated_predictions2)
# Evaluate combined model
accuracy_combined, precision_combined, recall_combined, f1_combined = evaluate_model(y_test, combined_predictions)
# Print metrics
print("Logistic Regression Bagging:")
print(f"Accuracy: {accuracy1}, Precision: {precision1}, Recall: {recall1}, F1 Score: {f11}")
print("\nDecision Tree Bagging:")
print(f"Accuracy: {accuracy2}, Precision: {precision2}, Recall: {recall2}, F1 Score: {f12}")
print("\nCombined Model:")
print(f"Accuracy: {accuracy_combined}, Precision: {precision_combined}, Recall: {recall_combined}, F1 Score: {f1_combined}")
 









2.boosting.py







[11:49 AM] Poornima (Guest)
# GBM classifier
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
# Load the dataset
iris = load_iris()
X, y = iris.data, iris.target
# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

# Define a simple neural network as a weak learner
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(X_train.shape[1], 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Train a weak learner
def train_model(model, X_train, y_train, epochs=100):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
    return model

# Train multiple weak learners sequentially
num_models = 3
learning_rate = 0.1
models = []
residuals = y_train.clone()
for _ in range(num_models):
    model = Net()
    model = train_model(model, X_train, residuals)
    models.append(model)
    # Predict on the training data
    predictions = model(X_train).detach()
    # Update residuals
    residuals -= learning_rate * predictions
    #print(f'Residuals :  {residuals}')
# Function to make predictions with the GBM model
def predict(models, X, learning_rate):
    predictions = torch.zeros((X.shape[0], 1))
    for model in models:
        predictions += learning_rate * model(X).detach()
       # print(f'Pred : {predictions}')
    return predictions

# Make predictions on the test set
y_pred = predict(models, X_test, learning_rate)
# Evaluate the GBM model
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test.numpy(), y_pred.numpy())
r2 = r2_score(y_test.numpy(), y_pred.numpy())
print(f"MSE: {mse}")
print(f"R2 Score: {r2}")
 




[12:43 PM] Poornima (Guest)

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# Load the dataset
data = load_breast_cancer()
X, y = data.data, data.target
# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
# Create AdaBoost with Logistic Regression as the base estimator
log_reg_boost = AdaBoostClassifier(estimator=LogisticRegression(max_iter=10000),
                                   n_estimators=50, random_state=42)
# Train the model
log_reg_boost.fit(X_train, y_train)
# Make predictions
log_reg_preds = log_reg_boost.predict(X_test)
# Evaluate the model
accuracy_lr = accuracy_score(y_test, log_reg_preds)
precision_lr = precision_score(y_test, log_reg_preds)
recall_lr = recall_score(y_test, log_reg_preds)
f1_lr = f1_score(y_test, log_reg_preds)
print("Logistic Regression Boosting:")
print(f"Accuracy: {accuracy_lr}, Precision: {precision_lr}, Recall: {recall_lr}, F1 Score: {f1_lr}")

from sklearn.tree import DecisionTreeClassifier
# Create AdaBoost with Decision Tree as the base estimator
dt_boost = AdaBoostClassifier(estimator=DecisionTreeClassifier(),
                              n_estimators=50, random_state=42)
# Train the model
dt_boost.fit(X_train, y_train)
# Make predictions
dt_preds = dt_boost.predict(X_test)
# Evaluate the model
accuracy_dt = accuracy_score(y_test, dt_preds)
precision_dt = precision_score(y_test, dt_preds)
recall_dt = recall_score(y_test, dt_preds)
f1_dt = f1_score(y_test, dt_preds)
print("\nDecision Tree Boosting:")
print(f"Accuracy: {accuracy_dt}, Precision: {precision_dt}, Recall: {recall_dt}, F1 Score: {f1_dt}")







 




3.XGBoost.py






[3:07 PM] Poornima (Guest)
# XGBoost classifier
import xgboost as xgb
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# Load the dataset
data = load_breast_cancer()
X, y = data.data, data.target
# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Convert the dataset into DMatrix, the data structure used by XGBoost
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)
# Set up the parameters
params = {
    'max_depth': 3,                     # Maximum depth of the tree
    'eta': 0.1,                         # Learning rate
    'objective': 'binary:logistic',     # Binary classification
    'eval_metric': 'logloss'            # Evaluation metric
}
# Train the XGBoost model
num_round = 10
bst = xgb.train(params, dtrain, num_round)
# Predict on the test set
preds = bst.predict(dtest)
preds = np.round(preds)  # Convert probabilities to binary predictions
# Evaluate the model
accuracy = accuracy_score(y_test, preds)
print(f"Accuracy: {accuracy}")
# Save the model
bst.save_model('xgboost_model.json')
# Load the model (example of loading)
loaded_bst = xgb.Booster()
loaded_bst.load_model('xgboost_model.json')
# Predict using the loaded model
loaded_preds = loaded_bst.predict(dtest)
loaded_preds = np.round(loaded_preds)
loaded_accuracy = accuracy_score(y_test, loaded_preds)
print(f"Loaded model accuracy: {loaded_accuracy}")





 




4.LightGBM.py





[3:50 PM] Poornima (Guest)
# LightGBM classifier
import lightgbm as lgb
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# Load the dataset
data = load_breast_cancer()
X, y = data.data, data.target
# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create the LightGBM dataset
dtrain = lgb.Dataset(X_train, label=y_train)
dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)
# Set up the parameters with adjusted num_leaves and other parameters
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 30,       # Increase the number of leaves
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'min_data_in_leaf': 20,
    'min_sum_hessian_in_leaf': 0.001
}

# Train the LightGBM model
num_round = 20                # Increase the number of boosting rounds
bst = lgb.train(params, dtrain, num_round, valid_sets=[dtrain, dtest])
# Predict on the test set
preds = bst.predict(X_test, num_iteration=bst.best_iteration)
preds = np.round(preds)  # Convert probabilities to binary predictions
# Evaluate the model
accuracy = accuracy_score(y_test, preds)
print(f"Accuracy: {accuracy}")
# Save the model
bst.save_model('lightgbm_model.txt')
# Load the model (example of loading)
loaded_bst = lgb.Booster(model_file='lightgbm_model.txt')
# Predict using the loaded model
loaded_preds = loaded_bst.predict(X_test, num_iteration=loaded_bst.best_iteration)
loaded_preds = np.round(loaded_preds)
loaded_accuracy = accuracy_score(y_test, loaded_preds)
print(f"Loaded model accuracy: {loaded_accuracy}")

'''
Contents of 'lightgbm_model.txt'
Model Parameters:
The file will contain the parameters used to train the LightGBM model. These parameters
include various settings.
Feature Names:
If provided during the creation of lgb.Dataset, the file may include the names of the 
features used in training (dtrain.feature_name). This helps in understanding which
features correspond to which columns in the input data.
Tree Structures:
For tree-based models (such as Gradient Boosting Decision Trees, GBDT), the file will 
include the structure of the trees that were trained during the boosting rounds. 
Each tree's structure typically includes nodes, splits, thresholds, and leaf values. 
This information is crucial for making predictions and understanding how the model makes
decisions.
Model Metadata:
Additional metadata about the model, such as the number of boosting rounds (num_round), 
the best iteration based on early stopping (bst.best_iteration), and other relevant 
information that might be useful when loading and using the model.
Format:
The file format is typically text-based and readable. It's designed to store all necessary 
information required to reconstruct and use the trained model later. LightGBM uses a custom
format optimized for its models, ensuring efficient storage and retrieval of model 
information.
'''
 


