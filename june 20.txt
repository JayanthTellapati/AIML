Hyperparameter Tuning








1.grid_search.py





# Hyperparameter Tuning with Grid Search
import torch
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()

# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor,
                                                    test_size=0.2, random_state=42)


# Define a simple neural network model for classification
class SimpleNN(torch.nn.Module):
    def __init__(self, input_size, num_classes, hidden_size=16):
        super(SimpleNN, self).__init__()
        self.fc1 = torch.nn.Linear(input_size, hidden_size)
        self.fc2 = torch.nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x


# Define parameter grid for hyperparameter tuning
param_grid = {
    'lr': [0.01, 0.001],
    'batch_size': [16, 32],
    'epochs': [50, 100]
}

# Implementing Grid Search
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import accuracy_score


# Function to train the model
def train_model(model, criterion, optimizer, dataloader):
    model.train()
    for inputs, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()


# Function to evaluate the model
def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.numpy())
            all_labels.extend(labels.numpy())
    return accuracy_score(all_labels, all_preds)


best_params = None
best_score = 0
results = []
best_params_list = []

# Perform grid search
for params in ParameterGrid(param_grid):
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)

    model = SimpleNN(input_size=X_tensor.shape[1], num_classes=len(torch.unique(y_tensor)))
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])

    # Train the model
    for epoch in range(params['epochs']):
        train_model(model, criterion, optimizer, train_loader)

    # Evaluate the model
    accuracy = evaluate_model(model, test_loader)

    # Store the results
    results.append((params, accuracy))
    print(f' Res :  \n {results}')

    '''if accuracy > best_score:
        best_score = accuracy
        best_params = params


# Print all results
for params, accuracy in results:
    print(f'Params: {params} => Accuracy: {accuracy:.4f}')

print(f'Best parameters: {best_params}')
print(f'Best accuracy: {best_score:.4f}')
'''

    # To print all possible best results
    if accuracy > best_score:
        best_score = accuracy
        best_params_list = [params]
    elif accuracy == best_score:
        best_params_list.append(params)

# Print all results
for params, accuracy in results:
    print(f'Params: {params} => Accuracy: {accuracy:.4f}')

print(f'Best accuracy: {best_score:.4f}')
print('Best parameter combinations:')
for params in best_params_list:
    print(params)










2.random_search.py










[1:00 PM] Poornima (Guest)
import torch
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()
# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)

# Model and Parameter Distribution
class SimpleNN(torch.nn.Module):
    def __init__(self, input_size, num_classes, hidden_size=16):
        super(SimpleNN, self).__init__()
        self.fc1 = torch.nn.Linear(input_size, hidden_size)
        self.fc2 = torch.nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Define parameter distribution for random search
param_dist = {
    'lr': [0.01, 0.001, 0.0001],
    'batch_size': [16, 32, 64],
    'epochs': [50, 100, 150]
}
# Implementing Random Search
from random import choice
from sklearn.metrics import accuracy_score

# Function to train the model
def train_model(model, criterion, optimizer, dataloader):
    model.train()
    for inputs, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Function to evaluate the model
def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.numpy())
            all_labels.extend(labels.numpy())
    return accuracy_score(all_labels, all_preds)

# Random search
n_iter = 5
best_params = None
best_score = 0
results = []
for _ in range(n_iter):
    params = {key: choice(value) for key, value in param_dist.items()}
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)
    model = SimpleNN(input_size=X_tensor.shape[1], num_classes=len(torch.unique(y_tensor)))
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])
    # Train the model
    for epoch in range(params['epochs']):
        train_model(model, criterion, optimizer, train_loader)
    # Evaluate the model
    accuracy = evaluate_model(model, test_loader)
    # Store the results
    results.append((params, accuracy))
    if accuracy > best_score:
        best_score = accuracy
        best_params = params
# Print all results
for params, accuracy in results:
    print(f'Params: {params} => Accuracy: {accuracy:.4f}')
print(f'Best parameters: {best_params}')
print(f'Best accuracy: {best_score:.4f}')









3.bayesian_optimization.py







Poornima (Guest)
3:34 PM

import torch
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from skopt.utils import use_named_args
from torch.utils.data import DataLoader, TensorDataset
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()
# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)
# Model and Optimization Process
# Define a simple neural network model for classification
class SimpleNN(torch.nn.Module):
    def __init__(self, input_size, num_classes, hidden_size=16):
        super(SimpleNN, self).__init__()
        self.fc1 = torch.nn.Linear(input_size, hidden_size)
        self.fc2 = torch.nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
from skopt.space import Integer, Real
# Define the search space for Bayesian optimization
bayesian_space = [
    Real(0.0001, 0.01, prior='log-uniform', name='lr'),
    Integer(16, 64, name='batch_size'),
    Integer(50, 150, name='epochs')
]
# Function to train the model
def train_model(model, criterion, optimizer, dataloader):
    model.train()
    for inputs, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
# Function to evaluate the model
def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.numpy())
            all_labels.extend(labels.numpy())
    return accuracy_score(all_labels, all_preds)
# Implementing Bayesian Optimization
from skopt import gp_minimize
# Define the objective function for Bayesian optimization
@use_named_args(bayesian_space)
def objective(lr, batch_size, epochs):
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=int(batch_size), shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=int(batch_size), shuffle=False)
    model = SimpleNN(input_size=X_tensor.shape[1], num_classes=len(torch.unique(y_tensor)))
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    # Train the model
    for epoch in range(epochs):
        train_model(model, criterion, optimizer, train_loader)
    # Evaluate the model
    accuracy = evaluate_model(model, test_loader)
    # Return the negative accuracy (since we minimize the objective function)
    return -accuracy

# Perform Bayesian optimization
result = gp_minimize(objective, bayesian_space, n_calls=20, random_state=42)
best_params = {
    'lr': result.x[0],
    'batch_size': result.x[1],
    'epochs': result.x[2]
}
print(f'Best parameters: {best_params}')
print(f'Best accuracy: {-result.fun:.4f}')

 






4.hyperband.py







[4:55 PM] Poornima (Guest)
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score
from ray import tune
from ray.tune.schedulers import HyperBandScheduler
from ray.air import session
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()
# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)

# Define a simple neural network model for classification
class SimpleNN(nn.Module):
    def __init__(self, input_size, num_classes, hidden_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Define the training function to be used by Hyperband
def train_model(config):
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=int(config["batch_size"]), shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=int(config["batch_size"]), shuffle=False)
    model = SimpleNN(input_size=X_tensor.shape[1], num_classes=len(torch.unique(y_tensor)),
                     hidden_size=int(config["hidden_size"]))
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=config["lr"])
    for epoch in range(int(config["epochs"])):
        model.train()
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        # Evaluate the model at the end of each epoch
        model.eval()
        all_preds = []
        all_labels = []
        with torch.no_grad():
            for inputs, labels in test_loader:
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                all_preds.extend(preds.numpy())
                all_labels.extend(labels.numpy())
        accuracy = accuracy_score(all_labels, all_preds)
        session.report({"mean_accuracy": accuracy})

# Define the search space for Hyperband
search_space = {
    "hidden_size": tune.choice([8, 16, 32]),
    "lr": tune.loguniform(1e-4, 1e-2),
    "batch_size": tune.choice([16, 32, 64]),
    "epochs": tune.choice([5, 10, 15])
}
# Set up Hyperband scheduler
scheduler = HyperBandScheduler(metric="mean_accuracy", mode="max")
# Run Hyperband optimization
analysis = tune.run(
    train_model,
    config=search_space,
    num_samples=5,
    scheduler=scheduler
)
# Get the best hyperparameters
best_config = analysis.get_best_config(metric="mean_accuracy", mode="max")
print(f"Best hyperparameters: {best_config}")
 
 

 