ML_python



Regression



1.linear_regression



[10:24 AM] Poornima (Guest)

from statistics import mean
import numpy as np
from matplotlib import pyplot as plt
from matplotlib import style

def calculate_slope_intercept(xvalues, yvalues):
    m = (((mean(xvalues) * mean(yvalues)) - mean(xvalues * yvalues)) /
         ((mean(xvalues) * mean(xvalues)) - mean(xvalues * xvalues)))
    b = mean(yvalues) - m * mean(xvalues)
    return m, b

def linear_Regression():
    regression_line = [(m * x) + b for x in xvalues]
    style.use('ggplot')
    plt.title('Training Data & Regression Line')
    plt.scatter(xvalues, yvalues, color='#003F72', label='Training Data')
    plt.plot(xvalues, regression_line, label='Reg Line')
    plt.legend(loc='best')
    plt.show()

def test_data():
    predict_xvalue = 7
    predict_yvalue = (m * predict_xvalue) + b
    print('Test Data for x :     ', predict_xvalue, '    ', 'Test Data for y :     ', predict_yvalue)
    plt.title('Train & Test Value')
    plt.scatter(xvalues, yvalues, color='#003F72', label='data')
    plt.scatter(predict_xvalue, predict_yvalue, color='#ff0000', label='Predicted Value')
    plt.legend(loc='best')
    plt.show()

def validate_results():
    predict_xvalues = np.array([2.5, 3.5, 4.5, 5.5, 6.5], dtype=np.float64)
    predict_yvalues = [(m * x) + b for x in predict_xvalues]
    print('Validation Data Set')
    print('X values', predict_xvalues)
    print('Y values', predict_yvalues)

# driver
xvalues = np.array([1, 2, 3, 4, 5], dtype=np.int32)
yvalues = np.array([14, 24, 34, 44, 54], dtype=np.int32)
m, b = calculate_slope_intercept(xvalues, yvalues)
print('Slope :  ', m, 'Intercept :  ', b)
linear_Regression()
test_data()
validate_results()



[12:47 PM] Poornima (Guest)

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
#Preparing the Dataset
# Generate some example data
np.random.seed(42)
X = np.random.rand(100, 1)
y = 5 * X + 2
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).float()
# Defining the Model
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1)
    def forward(self, x):
        return self.linear(x)
model = LinearRegressionModel()

# Training the Model
# Define the loss function and the optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)
    # Backward pass and optimization
    loss.backward()
    optimizer.step()
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
# Evaluating the Model
model.eval()
with torch.no_grad():
    predicted = model(X_tensor).detach().numpy()
# Plotting the results
plt.scatter(X, y, label='Original data')
plt.plot(X, predicted, label='Fitted line', color='red')
plt.legend()
plt.show()
 









2.polynomial_regression.py




[3:00 PM] Poornima (Guest)

#Polynomial Regression with PyTorch
# Preparing the Dataset
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
# Generate some example data
np.random.seed(42)
X = np.random.rand(100, 1)
y =  2 * X**2 + 3 * X + 1 + np.random.randn(100, 1) * 0.2
# Convert to PyTorch tensors
#X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).float()
# Feature Engineering (Creating Polynomial Features)
# Create polynomial features
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
# Convert polynomial features to PyTorch tensor
X_poly_tensor = torch.from_numpy(X_poly).float()

# Defining and Training the Model
class PolynomialRegressionModel(nn.Module):
    def __init__(self):
        super(PolynomialRegressionModel, self).__init__()
        self.linear = nn.Linear(X_poly.shape[1], 1)  # Adjust input size to match polynomial features
    def forward(self, x):
        return self.linear(x)
model = PolynomialRegressionModel()
# Define the loss function and the optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# Training loop
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    outputs = model(X_poly_tensor)
    loss = criterion(outputs, y_tensor)
    # Backward pass and optimization
    loss.backward()
    optimizer.step()
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Evaluating the Model
model.eval()
with torch.no_grad():
    predicted = model(X_poly_tensor).detach().numpy()
# Plotting the results
plt.scatter(X, y, label='Original data')
plt.scatter(X, predicted, label='Fitted curve', color='red')
plt.legend()
plt.show()









3.ridge_lasso_regression.py






[4:05 PM] Poornima (Guest)
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
# Generate some example data
np.random.seed(42)
X = np.random.rand(100, 1)
y = 2 * X + 1 + np.random.randn(100, 1) * 0.2
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).float()
print('Ridge Regression Epochs')
class RidgeRegressionModel(nn.Module):
    def __init__(self):
        super(RidgeRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1)
    def forward(self, x):
        return self.linear(x)
model = RidgeRegressionModel()
# Define the loss function with L2 regularization
criterion = nn.MSELoss()
lambda_reg = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# Training loop
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)

    # Add L2 regularization
    l2_reg = torch.tensor(0.)
    for param in model.parameters():
        l2_reg += torch.linalg.norm(param, 2)
    loss += lambda_reg * l2_reg
    # Backward pass and optimization
    loss.backward()
    optimizer.step()
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

#===============================================================
print('Lasso Regression Epochs')
class LassoRegressionModel(nn.Module):
    def __init__(self):
        super(LassoRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1)
    def forward(self, x):
        return self.linear(x)
model = LassoRegressionModel()
# Define the loss function with L1 regularization
criterion = nn.MSELoss()
lambda_reg = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# Training loop
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)
    # Add L1 regularization
    l1_reg = torch.tensor(0.)
    for param in model.parameters():
        l1_reg += torch.linalg.norm(param, 1)
    loss += lambda_reg * l1_reg
    # Backward pass and optimization
    loss.backward()
    optimizer.step()
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
 






4.ridge_lasso_reg_visualization.py




Poornima (Guest)
5:03 PM
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
# Generate some example data
np.random.seed(42)
X = np.random.rand(100, 1)
y = 2 * X + 1 + np.random.randn(100, 1) * 0.2
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).float()

class RidgeRegressionModel(nn.Module):
    def __init__(self):
        super(RidgeRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1)
    def forward(self, x):
        return self.linear(x)

model = RidgeRegressionModel()
# Define the loss function with L2 regularization
criterion = nn.MSELoss()
lambda_reg = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# Training loop
num_epochs = 1000
losses = []
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)
    # Add L2 regularization
    l2_reg = torch.tensor(0.)
    for param in model.parameters():
        l2_reg += torch.norm(param, 2)
    loss += lambda_reg * l2_reg
    # Backward pass and optimization
    loss.backward()
    optimizer.step()
    losses.append(loss.item())
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')
# Evaluation and Visualization
model.eval()
with torch.no_grad():
    predicted = model(X_tensor).detach().numpy()
# Plotting the results
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(range(num_epochs), losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.subplot(1, 2, 2)
plt.scatter(X, y, label='Original data')
plt.plot(X, predicted, label='Fitted line', color='red')
plt.legend()
plt.title('Ridge Regression Fit')
plt.show()
#===============================================================

class LassoRegressionModel(nn.Module):
    def __init__(self):
        super(LassoRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1)
    def forward(self, x):
        return self.linear(x)

model = LassoRegressionModel()
# Define the loss function with L1 regularization
criterion = nn.MSELoss()
lambda_reg = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# Training loop
num_epochs = 1000
losses = []
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    # Forward pass
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)
    # Add L1 regularization
    l1_reg = torch.tensor(0.)
    for param in model.parameters():
        l1_reg += torch.norm(param, 1)
    loss += lambda_reg * l1_reg
    # Backward pass and optimization
    loss.backward()
    optimizer.step()
    losses.append(loss.item())
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')
# Evaluation and Visualization
model.eval()
with torch.no_grad():
    predicted = model(X_tensor).detach().numpy()
# Plotting the results
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(range(num_epochs), losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.subplot(1, 2, 2)
plt.scatter(X, y, label='Original data')
plt.plot(X, predicted, label='Fitted line', color='red')
plt.legend()
plt.title('Lasso Regression Fit')
plt.show()

#========================================================

def calculate_metrics(y_true, y_pred):
    mae = torch.mean(torch.abs(y_true - y_pred)).item()
    mse = torch.mean((y_true - y_pred) ** 2).item()
    rmse = torch.sqrt(torch.mean((y_true - y_pred) ** 2)).item()
    ss_res = torch.sum((y_true - y_pred) ** 2)
    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)
    r2 = 1 - (ss_res / ss_tot).item()
    return mae, mse, rmse, r2
# Calculate metrics for Ridge Regression
model.eval()
with torch.no_grad():
    y_pred = model(X_tensor)
    mae, mse, rmse, r2 = calculate_metrics(y_tensor, y_pred)
    print(f'Ridge Regression - MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R-squared: {r2:.4f}')
# Calculate metrics for Lasso Regression
model.eval()
with torch.no_grad():
    y_pred = model(X_tensor)
    mae, mse, rmse, r2 = calculate_metrics(y_tensor, y_pred)
    print(f'Lasso Regression - MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R-squared: {r2:.4f}')
 

 
 