Dimensionality Reduction





1.PCA.py






[12:09 PM] Poornima (Guest)
# PCA with PyTorch
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
# Preparing the Dataset
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()

# Computing the Covariance Matrix
# Standardize the dataset
X_mean = X_tensor.mean(dim=0)
X_std = X_tensor.std(dim=0)
X_standardized = (X_tensor - X_mean) / X_std
#print('Mean, std & Stand ', X_mean, X_std, X_standardized)
# Compute the covariance matrix
cov_matrix = torch.mm(X_standardized.T, X_standardized) / (X_standardized.size(0) - 1)
print('Cov Mat \n',cov_matrix)
# Performing Eigen Decomposition
# Perform eigen decomposition using torch.linalg.eig
eigenvalues_complex, eigenvectors_complex = torch.linalg.eig(cov_matrix)
print('Eig val, Eig vec ', eigenvalues_complex, eigenvectors_complex)
# Extract the real part of the eigenvalues and eigenvectors
eigenvalues = eigenvalues_complex.real
eigenvectors = eigenvectors_complex.real
# Projecting Data onto the New Feature Space
# Sort eigenvalues and corresponding eigenvectors
sorted_indices = torch.argsort(eigenvalues, descending=True)
sorted_eigenvectors = eigenvectors[:, sorted_indices]
print('Sorted EVec', sorted_eigenvectors)
# Select the top k eigenvectors
k = 2
top_k_eigenvectors = sorted_eigenvectors[:, :k]
# Project the data onto the new feature space
X_reduced = torch.mm(X_standardized, top_k_eigenvectors)
# Visualizing the Reduced Dimensionality Data
# Convert to NumPy for plotting
X_reduced_np = X_reduced.detach().numpy()
y_np = y
# Plot the reduced dimensionality data
plt.figure(figsize=(8, 6))
for target in np.unique(y_np):
    indices = y_np == target
    plt.scatter(X_reduced_np[indices, 0], X_reduced_np[indices, 1], label=data.target_names[target])
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Iris Dataset')
plt.legend()
plt.show()






2.t-SNE.py





[2:45 PM] Poornima (Guest)
# t-SNE with PyTorch
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
# Preparing the Dataset
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()

# Defining Parameters (Perplexity, Learning Rate)
perplexity = 7
learning_rate = 200
num_iterations = 1000

# Running the t-SNE Algorithm
from sklearn.manifold import TSNE
# Run t-SNE
tsne = TSNE(n_components=2, perplexity=perplexity,
            learning_rate=learning_rate, n_iter=num_iterations, random_state=42)
X_tsne = tsne.fit_transform(X)
# Convert to PyTorch tensor
X_tsne_tensor = torch.from_numpy(X_tsne).float()
# Visualizing the Reduced Dimensionality Data
# Plot the reduced dimensionality data
plt.figure(figsize=(8, 6))
for target in np.unique(y):
    indices = y == target
    plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], label=data.target_names[target])
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.title('t-SNE of Iris Dataset')
plt.legend()
plt.show()

 





3.LDA.py






[3:30 PM] Poornima (Guest)
# Implementing LDA with PyTorch
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
# Preparing the Dataset
# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target
# Convert to PyTorch tensors
X_tensor = torch.from_numpy(X).float()
y_tensor = torch.from_numpy(y).long()

# Computing Scatter Matrices
# Compute the overall mean
mean_overall = X_tensor.mean(dim=0)
# Compute the within-class scatter matrix
classes = torch.unique(y_tensor)
print('Classes :', classes)
S_W = torch.zeros((X_tensor.size(1), X_tensor.size(1)))
for c in classes:
    X_c = X_tensor[y_tensor == c]
    mean_c = X_c.mean(dim=0)
    S_W += torch.mm((X_c - mean_c).T, (X_c - mean_c))
# Compute the between-class scatter matrix
S_B = torch.zeros((X_tensor.size(1), X_tensor.size(1)))
for c in classes:
    N_c = X_tensor[y_tensor == c].size(0)
    mean_c = X_tensor[y_tensor == c].mean(dim=0)
    mean_diff = (mean_c - mean_overall).unsqueeze(1)
    S_B += N_c * torch.mm(mean_diff, mean_diff.T)

# Performing Eigen Decomposition
# Compute the matrix S_W^{-1} S_B
eigvals, eigvecs = torch.linalg.eig(torch.mm(torch.linalg.inv(S_W), S_B))
# Extract the real part of the eigenvalues and eigenvectors
eigvals = eigvals.real
eigvecs = eigvecs.real

# Projecting Data onto the New Feature Space
# Sort eigenvalues and corresponding eigenvectors
sorted_indices = torch.argsort(eigvals, descending=True)
sorted_eigvecs = eigvecs[:, sorted_indices]
# Select the top k eigenvectors
k = 2
top_k_eigvecs = sorted_eigvecs[:, :k]
# Project the data onto the new feature space
X_lda = torch.mm(X_tensor, top_k_eigvecs)

# Visualizing the Reduced Dimensionality Data
# Convert to NumPy for plotting
X_lda_np = X_lda.detach().numpy()
y_np = y
# Plot the reduced dimensionality data
plt.figure(figsize=(8, 6))
for target in np.unique(y_np):
    indices = y_np == target
    plt.scatter(X_lda_np[indices, 0], X_lda_np[indices, 1], label=data.target_names[target])
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.title('LDA of Iris Dataset')
plt.legend()
plt.show()


 



 